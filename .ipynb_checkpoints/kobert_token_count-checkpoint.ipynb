{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87bce323-ee6d-4b7b-930f-23b6530332c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# 경로 설정\n",
    "input_file_path = 'inference_text/AA/wiki_00'\n",
    "output_file_path = 'new_truncation_tokens.txt'\n",
    "\n",
    "# KoBERT 토크나이저 로드\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "# 최대 시퀀스 길이 설정\n",
    "max_length = 512\n",
    "\n",
    "def process_text(text):\n",
    "    # 텍스트 인코딩\n",
    "    encoded = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "    return encoded, decoded\n",
    "\n",
    "def main():\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # </doc> 구분자로 텍스트 분할\n",
    "    documents = content.split('</doc>')\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for doc in documents:\n",
    "            if doc.strip():  # 빈 텍스트 무시\n",
    "                original_text = doc.strip()\n",
    "                encoded, decoded = process_text(original_text)\n",
    "\n",
    "                # 원본 텍스트와 디코딩된 텍스트의 길이 차이 계산\n",
    "                original_length = len(original_text)\n",
    "                decoded_length = len(decoded)\n",
    "                length_difference = abs(original_length - decoded_length)\n",
    "\n",
    "                # 길이 차이가 10 이상인 경우 파일에 기록\n",
    "                if length_difference >= 10:\n",
    "                    output_file.write(\"Original Text:\\n\")\n",
    "                    output_file.write(original_text + \"\\n\\n\")\n",
    "                    output_file.write(\"Encoded and Decoded Text:\\n\")\n",
    "                    output_file.write(decoded + \"\\n\\n\")\n",
    "                    output_file.write(f\"Length Difference: {length_difference}\\n\")\n",
    "                    output_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84865220-cf53-4c11-bae5-4be3c5327d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc307ede-e6ea-4afe-8524-2dd0548f33e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file_path = 'dataset/question.json'\n",
    "output_file_path = 'new_truncation_tokens2.txt'\n",
    "\n",
    "# KoBERT 토크나이저 로드\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "# 최대 시퀀스 길이 설정\n",
    "max_length = 512\n",
    "\n",
    "def process_text(text):\n",
    "    # 텍스트 인코딩\n",
    "    encoded = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "    return encoded, decoded\n",
    "\n",
    "def main():\n",
    "    # JSON 파일 읽기\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    texts = data.get(\"text\", [])\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for text in texts:\n",
    "            if text.strip():  # 빈 텍스트 무시\n",
    "                encoded, decoded = process_text(text.strip())\n",
    "\n",
    "                # 원본 텍스트와 디코딩된 텍스트의 길이 차이 계산\n",
    "                original_length = len(text.strip())\n",
    "                decoded_length = len(decoded)\n",
    "                length_difference = abs(original_length - decoded_length)\n",
    "\n",
    "                # 길이 차이가 10 이상인 경우 파일에 기록\n",
    "                if length_difference >= 20:\n",
    "                    output_file.write(\"Original Text:\\n\")\n",
    "                    output_file.write(text.strip() + \"\\n\\n\")\n",
    "                    output_file.write(\"Encoded and Decoded Text:\\n\")\n",
    "                    output_file.write(decoded + \"\\n\\n\")\n",
    "                    output_file.write(f\"Length Difference: {length_difference}\\n\")\n",
    "                    output_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08dbc15b-219c-45fd-982a-73efc9248557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968efaf2-7479-4c37-88b7-67fa533a5ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# 경로 설정\n",
    "input_file_path = 'text/AA/wiki_00'\n",
    "output_file_path = 'new_truncation_tokens3.txt'\n",
    "\n",
    "# KoBERT 토크나이저 로드\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "# 최대 시퀀스 길이 설정\n",
    "max_length = 512\n",
    "\n",
    "def process_text(text):\n",
    "    # 텍스트 인코딩\n",
    "    encoded = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "    return encoded, decoded\n",
    "\n",
    "def main():\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # </doc> 구분자로 텍스트 분할\n",
    "    documents = content.split('</doc>')\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for doc in documents:\n",
    "            if doc.strip():  # 빈 텍스트 무시\n",
    "                original_text = doc.strip()\n",
    "                encoded, decoded = process_text(original_text)\n",
    "\n",
    "                # 원본 텍스트와 디코딩된 텍스트의 길이 차이 계산\n",
    "                original_length = len(original_text)\n",
    "                decoded_length = len(decoded)\n",
    "                length_difference = abs(original_length - decoded_length)\n",
    "\n",
    "                # 길이 차이가 10 이상인 경우 파일에 기록\n",
    "                if length_difference >= 30:\n",
    "                    output_file.write(\"Original Text:\\n\")\n",
    "                    output_file.write(original_text + \"\\n\\n\")\n",
    "                    output_file.write(\"Encoded and Decoded Text:\\n\")\n",
    "                    output_file.write(decoded + \"\\n\\n\")\n",
    "                    output_file.write(f\"Length Difference: {length_difference}\\n\")\n",
    "                    output_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e51f97c-15ec-47c9-921f-d89ae3092807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a33d1a-0a3a-4a93-aa40-f2c3906b103e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
